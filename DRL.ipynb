{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims = (32,32), activation =nn.ReLU):\n",
    "        super(QNetwork,self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim,hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layer = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.hidden_layer.append(nn.Linear(hidden_dims[i],hidden_dims[i+1]))\n",
    "            self.hidden_layer.append(activation())\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(*self.hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x= state\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x= self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, buffer_size=10000, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.q_network(state_tensor)).item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        \n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward = 13.0\n",
      "Episode 1: Total Reward = 27.0\n",
      "Episode 2: Total Reward = 14.0\n",
      "Episode 3: Total Reward = 14.0\n",
      "Episode 4: Total Reward = 16.0\n",
      "Episode 5: Total Reward = 14.0\n",
      "Episode 6: Total Reward = 15.0\n",
      "Episode 7: Total Reward = 14.0\n",
      "Episode 8: Total Reward = 23.0\n",
      "Episode 9: Total Reward = 15.0\n",
      "done\n",
      "[13.0, 27.0, 14.0, 14.0, 16.0, 14.0, 15.0, 14.0, 23.0, 15.0]\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "episodes = 10\n",
    "update_target_every = 10\n",
    "rewards_history = []\n",
    "frames = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.train_step()\n",
    "        \n",
    "        # frame = env.render()\n",
    "        # frames.append(frame)\n",
    "    \n",
    "    rewards_history.append(total_reward)\n",
    "    agent.decay_epsilon()\n",
    "    if episode % update_target_every == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "print(\"done\")\n",
    "env.close()\n",
    "print(rewards_history)\n",
    "# Plot rewards\n",
    "# plt.plot(rewards_history)\n",
    "# print('really??')\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.title(\"DQN Training Progress\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b31fdf5820>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.plot(rewards_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save video of agent performance\n",
    "# height, width, layers = frames[0].shape\n",
    "# video = cv2.VideoWriter('dqn_training.avi', cv2.VideoWriter_fourcc(*'XVID'), 30, (width, height))\n",
    "\n",
    "# for frame in frames:\n",
    "#     video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# video.release()\n",
    "# print(\"Training video saved as dqn_training.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
